# dl_dz_1

## Эксперимент 1

| Training epochs | Val Loss | Test Loss | Test ROC-AUC |
| --------------- | -------- | --------- | ------------ |
|        75       |  0.2296  |  0.2148   |    0.9192    |


Модель довольно долго сходится, но при этом все равно в итоге достигает хороших значений метрики на тесте. Оптимальное кол-во эпох выбрал равным 75, т.к. лосс на валидации убывал вплоть до 75 эпохи.


## Эксперимент 2

| Training epochs | Val Loss | Test Loss | Test ROC-AUC |
| --------------- | -------- | --------- | ------------ |
|        59       |  0.2305  |   0.1859   |    0.9346    |

Модель сошлась быстрее, лосс оказался больше, чем в первом эксперименте, но тест лосс и рок аук получились меньше. После 59 эпохи модель начинает переобучаться.


## Эксперимент 3
| Training epochs | Val Loss | Test Loss | Test ROC-AUC |
| --------------- | -------- | --------- | ------------ |
|        17       |  0.2364  |   0.2528  |    0.9331    |


Модель сошлась гораздо быстрее, чем перввые две, но по метрике на тесте показала значения хуже, чем предыдущая сеть.


## Эксперимент 4

| Dropout | Val Loss | Test Loss | Test ROC-AUC 
| ------- | -------- | --------- | ------------ |
|  0.01   |  0.2432  |   0.1925  |    0.9468    |
|  0.1    |  0.2310  |   0.1829  |    0.9395    |
|  0.2    |  0.2296  |   0.1999  |    0.9287    |
|  0.5    |  0.2566  |   0.2445  |    0.9111    |
|  0.9    |  0.3419  |   0.3382  |    0.8603    |


Значения дропаут напрямую влияет на качество модели, т.к. при больших значениях вероятности дропаута (0.5, 0.9) модель отрабатывает значительно хуже, чем без дропаута, т.к. большая часть нейронов в слоях зануляются, при этом маленькие значения дропаута (0.01, 0.1, 0.2) мешают переобучению модели и тем самым бустят сетку до 0.9468 рок аук на тесте.

## Эксперимент 5

| Lr   | Weight Decay|Val Loss | Test Loss | Test ROC-AUC 
| --   | ------------- |-------- | --------| ------------ |
| 0.1  |  0.1          |  0.2827 |  0.2997 |    0.8732    |
| 0.1  | 0.01          |  0.2264 |  0.2219 |    0.9145    |
| 0.1  | 0.001         |  0.2372 |  0.1832 |    0.9451    |
| 0.05 |  0.1          |  0.2621 |  0.2986 |    0.8753    |
| 0.05 | 0.01          |  0.2270 |  0.1969 |    0.9301    |
| 0.05 | 0.001         |  0.2302 |  0.1768 |    0.9468    |
| 0.01 |  0.1          |  0.2391 |  0.2419 |    0.9215    |
| 0.01 | 0.01          |  0.2323 |  0.1856 |    0.9387    |
| 0.01 | 0.001         |  0.2315 |  0.1834 |    0.9399    |


При больших значениях лямбда и lr модель показывает себя хуже всего (0.8732 худшее значение метрики на тесте). При самом маленькой лямбде и значениях lr 0.05/0.01 модель показывает лучшие результаты (0.9468, 0.9399). При этом удивительно, что при lr равном 0.1 и лямбда равном 0.001 достигается второй мвксимум рокаука на тесте, хотя обычно использование больших значений lr может приводить к плохим результатам. Большие значения лямбда же зачастую могут способствовать недообучению, поэтому все результаты при лямбда равном 0.1 значительнохуже остальных.



P.S все эксперименты, графики лоссов и т.д. в ноутбуке dl_dz_1.ipynb
